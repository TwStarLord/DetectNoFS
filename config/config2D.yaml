model:
  batch_size: 1
  learning_rate: 1e-5
  num_epochs: 100

  hidden_dim: 96
  layers: (2,2,6,2)
  heads: (3,6,12,24)
  channels: 3
  num_classes: 5
  head_dim: 32
  window_size: 7
  downscaling_factors: (4, 2, 2, 2)
  relative_pos_embedding: True

blocks:
  #  ca start
  ca: "MutilHeadCrossAttentionBlock"
#  ca: "MutilHeadSelfAttentionBlock"
  #  ca end

  #  cls start
  cls_head: "GeMClassifier"
#  cls_head: "LiteTransformerPoolCls"
#  cls_head: "MixPoolClassifier"
#  cls_head: "PatchTransformerPoolCls"
#  cls_head: "SAOLPoolCls"
#  cls_head: "SPPoolCls"
#  cls_head: "TransformerPoolCls"
  #  cls end

  #  DIFB start
  DIFB: "DIFB2D"
#  DIFB: "DIFB2D_v1"
  #  DIFB end

  #  feature_fusion start
  feature_fusion: "AFFFeatureFusion"
#  feature_fusion: "ConvFeatureFsuion"
  # feature_fusion end

  #  upsample start
  upsample: "PatchExpandConvTranspose"
#  upsample: "PatchExpandPixelShuffle"
#  upsample: "PatchExpandUpsample"
  #  upsample end